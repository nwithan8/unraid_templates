<?xml version="1.0"?>
<Container version="2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="https://raw.githubusercontent.com/nwithan8/unraid_templates/main/templates/template_schema.xsd template_schema.xsd">
    <Name>llama.cpp</Name>
    <Repository>ghcr.io/ggml-org/llama.cpp:full</Repository>
    <Registry>ghcr.io/ggml-org/llama.cpp</Registry>
    <Branch>
        <Tag>full</Tag>
        <TagDescription>Latest full release</TagDescription>
    </Branch>
    <Branch>
        <Tag>light</Tag>
        <TagDescription>Latest light release</TagDescription>
    </Branch>
    <Branch>
        <Tag>server</Tag>
        <TagDescription>Latest server-only release</TagDescription>
    </Branch>
    <Branch>
        <Tag>full-cuda</Tag>
        <TagDescription>Latest full release with CUDA support</TagDescription>
    </Branch>
    <Branch>
        <Tag>light-cuda</Tag>
        <TagDescription>Latest light release with CUDA support</TagDescription>
    </Branch>
    <Branch>
        <Tag>server-cuda</Tag>
        <TagDescription>Latest server-only release with CUDA support</TagDescription>
    </Branch>
    <Branch>
        <Tag>full-rocm</Tag>
        <TagDescription>Latest full release with ROCm support</TagDescription>
    </Branch>
    <Branch>
        <Tag>light-rocm</Tag>
        <TagDescription>Latest light release with ROCm support</TagDescription>
    </Branch>
    <Branch>
        <Tag>server-rocm</Tag>
        <TagDescription>Latest server-only release with ROCm support</TagDescription>
    </Branch>
    <Branch>
        <Tag>full-musa</Tag>
        <TagDescription>Latest full release with MUSA support</TagDescription>
    </Branch>
    <Branch>
        <Tag>light-musa</Tag>
        <TagDescription>Latest light release with MUSA support</TagDescription>
    </Branch>
    <Branch>
        <Tag>server-musa</Tag>
        <TagDescription>Latest server-only release with MUSA support</TagDescription>
    </Branch>
    <Branch>
        <Tag>full-intel</Tag>
        <TagDescription>Latest full release with SYCL support</TagDescription>
    </Branch>
    <Branch>
        <Tag>light-intel</Tag>
        <TagDescription>Latest light release with SYCL support</TagDescription>
    </Branch>
    <Branch>
        <Tag>server-intel</Tag>
        <TagDescription>Latest server-only release with SYCL support</TagDescription>
    </Branch>
    <Branch>
        <Tag>full-vulkan</Tag>
        <TagDescription>Latest full release with Vulkan support</TagDescription>
    </Branch>
    <Branch>
        <Tag>light-vulkan</Tag>
        <TagDescription>Latest light release with Vulkan support</TagDescription>
    </Branch>
    <Branch>
        <Tag>server-vulkan</Tag>
        <TagDescription>Latest server-only release with Vulkan support</TagDescription>
    </Branch>
    <Network>bridge</Network>
    <WebUI>http://[IP]:[PORT:8000]/</WebUI>
    <PostArgs>-m /models/model.gguf --port 8000 --host 0.0.0.0 --n-gpu-layers 1</PostArgs>
    <Privileged>false</Privileged>
    <Support>https://github.com/ggml-org/llama.cpp/issues</Support>
    <Project>https://github.com/ggml-org/llama.cpp</Project>
    <Overview>
        Inference of Meta's LLaMA model (and others) in pure C/C++
    </Overview>
    <Beta>False</Beta>
    <Category>AI: Productivity: Tools: Other: Status:Stable</Category>
    <Icon>https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png</Icon>
    <Banner>https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png</Banner>
    <TemplateURL>https://raw.githubusercontent.com/nwithan8/unraid_templates/main/templates/llama_cpp.xml</TemplateURL>
    <Maintainer>
        <WebPage>https://github.com/nwithan8</WebPage>
    </Maintainer>
    <Requires>
        The image for this container is several gigabytes. If you receive a "no space left on device" warning during installation, please increase the vDisk size in your Docker settings. &#xD;
        This container expects a "model.gguf" file to be present in the model storage path. &#xD;
        If you are using an Nvidia GPU, add "--gpus all" to the Extra Parameters field under Advanced.
    </Requires>
    <Changes>
        ### 2026-02-23

        Fix repo location and branch options

        ### 2025-05-03

        Add MUSA branch

        ### 2024-05-07

        Initial release
    </Changes>
    <Config Name="WebUI" Target="8000" Default="8000" Mode="tcp" Description="Container Port: 8000" Type="Port" Display="always" Required="true" Mask="false">8000</Config>
    <Config Name="Model Storage Path" Target="/models" Default="/mnt/user/appdata/llama_cpp/model" Mode="rw" Description="Storage for model" Type="Path" Display="advanced" Required="true" Mask="false">/mnt/user/appdata/llama_cpp/model</Config>
</Container>
