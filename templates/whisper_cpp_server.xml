<?xml version="1.0"?>
<Container version="2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="https://raw.githubusercontent.com/nwithan8/unraid_templates/main/templates/template_schema.xsd template_schema.xsd">
    <Name>Whisper-CPP-Server</Name>
    <Repository>litongjava/whisper-cpp-server:1.0.0</Repository>
    <Registry>https://hub.docker.com/r/litongjava/whisper-cpp-server</Registry>
    <Network>bridge</Network>
    <Privileged>false</Privileged>
    <Support>https://github.com/litongjava/whisper-cpp-server/issues</Support>
    <Project>https://github.com/litongjava/whisper-cpp-server</Project>
    <Overview>
        Whisper-CPP-Server is a high-performance speech recognition service written in C++, designed to provide developers and enterprises with a reliable and efficient speech-to-text inference engine. This project implements technology from ggml to perform inference on the open-source Whisper model. While ensuring speed and accuracy, it supports pure CPU-based inference operations, allowing for high-quality speech recognition services without the need for specialized hardware accelerators.
    </Overview>
    <Beta>False</Beta>
    <Category>AI: Productivity: Tools: Other: Status:Stable</Category>
    <ExtraSearchTerms>whisper voice text speech openai ai transcription real-time model</ExtraSearchTerms>
    <Icon>https://raw.githubusercontent.com/nwithan8/unraid_templates/master/images/whisper-cpp-server-icon.png</Icon>
    <TemplateURL>https://raw.githubusercontent.com/nwithan8/unraid_templates/main/templates/whisper_cpp_server.xml</TemplateURL>
    <Maintainer>
        <WebPage>https://github.com/nwithan8</WebPage>
    </Maintainer>
    <Changes>
        ### 2025-05-07

        Initial release
    </Changes>
    <Requires>
        Expects a model file to be mounted in the Model File path BEFORE the container starts.
    </Requires>
    <PostArgs>/app/whisper_http_server_base_httplib -m /models/model.bin</PostArgs>
    <Config Name="API Port" Target="8080" Default="8080" Mode="tcp" Description="Container Port: 8080" Type="Port" Display="always-hide" Required="true" Mask="false">8080</Config>
    <Config Name="Model File" Target="/models/model.bin" Default="" Description="Path to the model file" Type="Path" Display="always-hide" Required="true" Mask="false"/>
</Container>
