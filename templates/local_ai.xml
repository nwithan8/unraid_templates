<?xml version="1.0"?>
<Container version="2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="https://raw.githubusercontent.com/nwithan8/unraid_templates/main/templates/template_schema.xsd template_schema.xsd">
    <Name>LocalAI</Name>
    <Repository>localai/localai:latest</Repository>
    <Registry>https://hub.docker.com/r/localai/localai</Registry>
    <Branch>
        <Tag>latest</Tag>
        <TagDescription>Latest standard release (CPU only)</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-gpu-nvidia-cuda-11</Tag>
        <TagDescription>Latest standard release with Nvidia GPU (CUDA 11) support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-gpu-nvidia-cuda-12</Tag>
        <TagDescription>Latest standard release with Nvidia GPU (CUDA 12) support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-nvidia-l4t-arm64</Tag>
        <TagDescription>Latest standard release with Nvidia Jetson (L4T) ARM64 support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-gpu-hipblas</Tag>
        <TagDescription>Latest standard release with AMD GPU support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-gpu-intel-f16</Tag>
        <TagDescription>Latest standard release with Intel iGPU (sycl f16) support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-gpu-intel-f32</Tag>
        <TagDescription>Latest standard release with Intel iGPU (sycl f32) support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-aio-cpu</Tag>
        <TagDescription>Latest all-in-one release (CPU only)</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-aio-gpu-nvidia-cuda-11</Tag>
        <TagDescription>Latest all-in-one release with Nvidia GPU (CUDA 11) support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-aio-gpu-nvidia-cuda-12</Tag>
        <TagDescription>Latest all-in-one release with Nvidia GPU (CUDA 12) support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-aio-gpu-hipblas</Tag>
        <TagDescription>Latest all-in-one release with AMD GPU support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-aio-gpu-intel-f16</Tag>
        <TagDescription>Latest all-in-one release with Intel iGPU (sycl f16) support</TagDescription>
    </Branch>
    <Branch>
        <Tag>latest-aio-gpu-intel-f32</Tag>
        <TagDescription>Latest all-in-one release with Intel iGPU (sycl f32) support</TagDescription>
    </Branch>
    <Network>bridge</Network>
    <WebUI>http://[IP]:[PORT:8080]/</WebUI>
    <Shell>bash</Shell>
    <Privileged>false</Privileged>
    <Support>https://forums.unraid.net/topic/133764-support-grtgbln-docker-templates</Support>
    <Project>https://localai.io/</Project>
    <Overview>
        The free, Open Source OpenAI alternative. Self-hosted, community-driven and local-first. &#xD;
        Drop-in replacement for OpenAI running on consumer-grade hardware. &#xD;
        No GPU required. &#xD;
        Runs gguf, transformers, diffusers and many more models architectures. &#xD;
        It allows to generate Text, Audio, Video, Images. Also with voice cloning capabilities. &#xD;
        &#xD;
        Additional image variants are also available: https://localai.io/basics/container/#standard-container-images &#xD;
        &#xD;
        For Nvidia GPU support, add "--gpus all" to the Extra Parameters field under Advanced. &#xD;
        For AMD GPU support, add "/dev/kfd" and "/dev/dri" each as a Device and add the required Variables: https://localai.io/features/gpu-acceleration/#setup-example-dockercontainerd &#xD;
        For Intel iGPU support, add "/dev/dri" as a Device and add "--device=/dev/dri" to the Extra Parameters field under Advanced. &#xD;
    </Overview>
    <Beta>False</Beta>
    <Category>AI: Productivity: Tools: Other: Status:Stable</Category>
    <Icon>https://github.com/go-skynet/LocalAI/assets/2420543/0966aa2a-166e-4f99-a3e5-6c915fc997dd</Icon>
    <TemplateURL>https://raw.githubusercontent.com/nwithan8/unraid_templates/main/templates/local_ai.xml</TemplateURL>
    <Maintainer>
        <WebPage>https://github.com/nwithan8</WebPage>
    </Maintainer>
    <Requires>
        The images for this container are several gigabytes (standard images upwards of ~17 GB, AIO images upwards of ~40 GB). &#xD;
        If you receive a "no space left on device" warning during installation, please increase the vDisk size in your Docker settings. &#xD;
        Additional image variants are also available: https://localai.io/basics/container/#standard-container-images &#xD;
    </Requires>
    <Changes>
        ### 2025-06-25

        Add new Docker tags

        ### 2024-06-20

        Update documentation for Intel iGPU passthrough

        ### 2024-06-15

        Add required GPU parameters by default, add non-AIO branches, add better size estimate in install warning

        ### 2024-06-07

        Remove broken GALLERIES environmental variable

        ### 2024-04-30

        Fix port mapping via bridge network

        ### 2024-04-20

        Initial release
    </Changes>
    <ExtraParams>--gpus=all</ExtraParams>
    <Config Name="WebUI" Target="8080" Default="8080" Mode="tcp" Description="Container Port: 8080" Type="Port" Display="always" Required="true" Mask="false">8080</Config>
    <Config Name="Debug mode" Target="DEBUG" Default="true|false" Description="Whether to enable debug mode" Type="Variable" Display="advanced" Required="true" Mask="false"/>
    <Config Name="Model Storage Path" Target="/build/models" Default="/mnt/user/appdata/local_ai/models" Mode="rw" Description="Storage for models" Type="Path" Display="advanced" Required="true" Mask="false">/mnt/user/appdata/local_ai/models</Config>
</Container>
